{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### **Stage 1: Preparing the dataset**","metadata":{"id":"YQvULiPHoQSa"}},{"cell_type":"code","source":"# Downloading and preparing the dataset\n\nimport urllib.request\nimport zipfile\nimport os\nfrom pathlib import Path\nimport pandas as pd\n\ndef download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n    if data_file_path.exists():\n        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n        return\n    with urllib.request.urlopen(url) as response: # Downloading the file\n        with open(zip_path, \"wb\") as out_file:\n            out_file.write(response.read())\n\n    with zipfile.ZipFile(zip_path, \"r\") as zip_ref: # Unzipping the file\n        zip_ref.extractall(extracted_path)\n\n    original_file_path=Path(extracted_path) / \"SMSSpamCollection\"\n    os.rename(original_file_path, data_file_path)\n    print(f\"File downloaded and saved as {data_file_path}\")\n\n# download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)","metadata":{"id":"UK8F0Mpybq0-","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:57:26.594944Z","iopub.execute_input":"2025-05-24T11:57:26.595221Z","iopub.status.idle":"2025-05-24T11:57:26.887859Z","shell.execute_reply.started":"2025-05-24T11:57:26.595194Z","shell.execute_reply":"2025-05-24T11:57:26.886975Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Creating a balanced dataset\n\ndef create_balanced_dataset(df):\n    num_spam=df[df[\"Label\"]==\"spam\"].shape[0] # Counts the instances of \"spam\"\n    ham_subset=df[df[\"Label\"]==\"ham\"].sample(num_spam, random_state=123) # Randomly samples \"ham\" instances to match the number of \"spam\" instances\n    balanced_df = pd.concat([\n        ham_subset, df[df[\"Label\"]==\"spam\"]\n    ])\n    return balanced_df","metadata":{"id":"aOTWn5JbedFL","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:57:26.889374Z","iopub.execute_input":"2025-05-24T11:57:26.889736Z","iopub.status.idle":"2025-05-24T11:57:26.894853Z","shell.execute_reply.started":"2025-05-24T11:57:26.889716Z","shell.execute_reply":"2025-05-24T11:57:26.893859Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Splitting the dataset\n\ndef random_split(df, train_frac, validation_frac):\n    df=df.sample(frac=1, random_state=123).reset_index(drop=True) # Shuffles the entire Dataframe\n    train_end=int(train_frac * len(df))\n    validation_end=train_end+int(validation_frac * len(df))\n\n    train_df=df[:train_end]\n    validation_df=df[train_end:validation_end]\n    test_df=df[validation_end:]\n\n    return train_df, validation_df, test_df","metadata":{"id":"gkcocIyifd_5","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:57:26.895705Z","iopub.execute_input":"2025-05-24T11:57:26.895895Z","iopub.status.idle":"2025-05-24T11:57:26.907278Z","shell.execute_reply.started":"2025-05-24T11:57:26.895880Z","shell.execute_reply":"2025-05-24T11:57:26.906562Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"url=\"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\nzip_path=\"sms_spam_collection.zip\"\nextracted_path=\"sms_spam_collection\"\ndata_file_path=Path(extracted_path) / \"SMSSpamCollection.tsv\"\n\ndownload_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\ndf=pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\nbalanced_df=create_balanced_dataset(df)\n\nbalanced_df[\"Label\"]=balanced_df[\"Label\"].map({\"ham\":0, \"spam\":1})\ntrain_df, validation_df, test_df=random_split(balanced_df, 0.7, 0.1)\n\ntrain_df.to_csv(\"train.csv\", index=None)\nvalidation_df.to_csv(\"validation.csv\", index=None)\ntest_df.to_csv(\"test.csv\", index=None)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZYsAVdaogcGb","outputId":"0df9a1e3-e19d-463e-b290-5665797945f7","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:57:26.908084Z","iopub.execute_input":"2025-05-24T11:57:26.908334Z","iopub.status.idle":"2025-05-24T11:57:27.379074Z","shell.execute_reply.started":"2025-05-24T11:57:26.908313Z","shell.execute_reply":"2025-05-24T11:57:27.378230Z"}},"outputs":[{"name":"stdout","text":"File downloaded and saved as sms_spam_collection/SMSSpamCollection.tsv\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass SpamDataset(Dataset):\n    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n        self.data=pd.read_csv(csv_file)\n\n        self.encoded_texts=[tokenizer.encode(text) for text in self.data[\"Text\"]]\n\n        if max_length is None:\n            self.max_length=self._longest_encoded_length()\n        else:\n            self.max_length=max_length\n            self.encoded_texts=[encoded_text[:self.max_length] for encoded_text in self.encoded_texts] # Truncates sequences if they are longer than max_length\n\n        self.encoded_texts=[encoded_text + [pad_token_id] * (self.max_length - len(encoded_text)) for encoded_text in self.encoded_texts]\n\n    def __getitem__(self, index):\n        encoded=self.encoded_texts[index]\n        label=self.data.iloc[index][\"Label\"]\n        return (\n            torch.tensor(encoded, dtype=torch.long),\n            torch.tensor(label, dtype=torch.long)\n        )\n    def __len__(self):\n        return len(self.data)\n\n    def _longest_encoded_length(self):\n        max_length=0\n        for encoded_text in self.encoded_texts:\n            encoded_length=len(encoded_text)\n            if encoded_length > max_length:\n                max_length=encoded_length\n        return max_length","metadata":{"id":"Za7TQbd3iYio","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:57:27.381327Z","iopub.execute_input":"2025-05-24T11:57:27.381540Z","iopub.status.idle":"2025-05-24T11:57:31.628533Z","shell.execute_reply.started":"2025-05-24T11:57:27.381522Z","shell.execute_reply":"2025-05-24T11:57:31.627947Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Instantiating PyTorch datasets\n\nimport tiktoken\n\ntokenizer=tiktoken.get_encoding(\"gpt2\")\n\ntrain_dataset=SpamDataset(\"train.csv\", max_length=None, tokenizer=tokenizer)\nval_dataset=SpamDataset(\"validation.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\ntest_dataset=SpamDataset(\"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)","metadata":{"id":"OkSDQRVykraN","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:57:31.629132Z","iopub.execute_input":"2025-05-24T11:57:31.629436Z","iopub.status.idle":"2025-05-24T11:57:33.126407Z","shell.execute_reply.started":"2025-05-24T11:57:31.629418Z","shell.execute_reply":"2025-05-24T11:57:33.125450Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Creating PyTorch dataloaders\n\nfrom torch.utils.data import DataLoader\n\ntorch.manual_seed(123)\n\nnum_workers=0\nbatch_size=8\n\ntrain_loader=DataLoader(\n    dataset=train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=num_workers,\n    drop_last=True,\n)\n\nval_loader=DataLoader(\n    dataset=val_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    drop_last=False,\n)\n\ntest_loader=DataLoader(\n    dataset=test_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    drop_last=False,\n)","metadata":{"id":"nomF5hkIlxG6","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:57:33.127405Z","iopub.execute_input":"2025-05-24T11:57:33.127710Z","iopub.status.idle":"2025-05-24T11:57:33.137378Z","shell.execute_reply.started":"2025-05-24T11:57:33.127676Z","shell.execute_reply":"2025-05-24T11:57:33.136700Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(\"Train loader:\")\nfor input_batch, target_batch in train_loader:\n    pass\n\nprint(f\"Input batch dimensions: {input_batch.shape}\")\nprint(f\"Label batch dimensions: {target_batch.shape}\")\n\nprint(f\"\\n{len(train_loader)} training batches\")\nprint(f\"{len(val_loader)} validation batches\")\nprint(f\"{len(test_loader)} test batches\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FNjN8yMJmnQv","outputId":"bde1ff54-b1d4-474d-d4cf-6e3e597c09fe","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:57:33.138107Z","iopub.execute_input":"2025-05-24T11:57:33.138403Z","iopub.status.idle":"2025-05-24T11:57:33.270128Z","shell.execute_reply.started":"2025-05-24T11:57:33.138384Z","shell.execute_reply":"2025-05-24T11:57:33.269599Z"}},"outputs":[{"name":"stdout","text":"Train loader:\nInput batch dimensions: torch.Size([8, 120])\nLabel batch dimensions: torch.Size([8])\n\n130 training batches\n19 validation batches\n38 test batches\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## **Stage 2: Initializing the model**","metadata":{"id":"jUfwzYvOoZAL"}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass LayerNorm(nn.Module):\n    def __init__(self, emb_dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n\n        return self.scale * norm_x + self.shift","metadata":{"id":"802uCAk7pETo","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:57:33.270701Z","iopub.execute_input":"2025-05-24T11:57:33.270907Z","iopub.status.idle":"2025-05-24T11:57:33.275900Z","shell.execute_reply.started":"2025-05-24T11:57:33.270892Z","shell.execute_reply":"2025-05-24T11:57:33.275198Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))","metadata":{"id":"GpuAtBxarIi3","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:57:33.276637Z","iopub.execute_input":"2025-05-24T11:57:33.277069Z","iopub.status.idle":"2025-05-24T11:57:33.286183Z","shell.execute_reply.started":"2025-05-24T11:57:33.277046Z","shell.execute_reply":"2025-05-24T11:57:33.285532Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n            GELU(),\n            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n        )\n\n    def forward(self, x):\n        return self.layers(x)","metadata":{"id":"BmGkKrbTpERM","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:57:33.286893Z","iopub.execute_input":"2025-05-24T11:57:33.287123Z","iopub.status.idle":"2025-05-24T11:57:33.298627Z","shell.execute_reply.started":"2025-05-24T11:57:33.287108Z","shell.execute_reply":"2025-05-24T11:57:33.297921Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        assert (d_out % num_heads == 0), \\\n            \"d_out must be divisible by num_heads\"\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out) # Linear layer to combine head outputs\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\n            \"mask\",\n            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n        )\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape # Shape: (batch, num_tokens, d_in)\n        queries = self.W_query(x)\n        keys = self.W_key(x)\n        values = self.W_value(x)\n\n        # We implicitly split the matrix by adding a 'num_heads' dimension\n        # Unroll last dim: (batch, num_tokens, d_out) -> (batch, num_tokens, num_tokens, head_dim)\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n\n        # Transpose: (batch, num_tokens, num_heads, head_dim) -> (batch, num_heads, num_tokens, head_dim)\n        queries = queries.transpose(1, 2)\n        keys = keys.transpose(1, 2)\n        values = values.transpose(1, 2)\n\n        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n        attn_scores = queries @ keys.transpose(2, 3) # Dot product for each head\n\n        # Original mask truncated to the number of tokens and converted to boolean\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n\n        # Use the mask to fill attention scores\n        attn_scores.masked_fill_(mask_bool, -torch.inf)\n\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Shape: (batch, num_tokens, num_heads, head_dim)\n        context_vec = (attn_weights @ values).transpose(1, 2)\n\n        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n        context_vec = self.out_proj(context_vec) # optional projection\n\n        return context_vec","metadata":{"id":"5m5-Y_fxpEOP","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:57:33.299288Z","iopub.execute_input":"2025-05-24T11:57:33.299491Z","iopub.status.idle":"2025-05-24T11:57:33.309609Z","shell.execute_reply.started":"2025-05-24T11:57:33.299476Z","shell.execute_reply":"2025-05-24T11:57:33.308915Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.att = MultiHeadAttention(\n            d_in = cfg[\"emb_dim\"],\n            d_out = cfg[\"emb_dim\"],\n            context_length = cfg[\"context_length\"],\n            num_heads = cfg[\"n_heads\"],\n            dropout = cfg[\"drop_rate\"],\n            qkv_bias = cfg[\"qkv_bias\"]\n        )\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n\n    def forward(self, x):\n        # Shortcut connection for attention block\n        shortcut = x\n        x = self.norm1(x)\n        x = self.att(x) # Shape [batch_size, num_tokens, emb_size]\n        x = self.drop_shortcut(x)\n        x = x + shortcut\n\n        # Shortcut connection for feed forward block\n        shortcut = x\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_shortcut(x)\n        x = x + shortcut # Add the original input back\n\n        return x","metadata":{"id":"Yn_9YECipELU","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:57:33.310511Z","iopub.execute_input":"2025-05-24T11:57:33.310915Z","iopub.status.idle":"2025-05-24T11:57:33.323185Z","shell.execute_reply.started":"2025-05-24T11:57:33.310896Z","shell.execute_reply":"2025-05-24T11:57:33.322575Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass GPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n        self.trf_blocks = nn.Sequential(\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n        )\n        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False)\n\n    def forward(self, in_idx):\n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n\n        # The device setting will allow us to train the model on a CPU or GPU, depending on which device the input data sits on.\n        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n        x = tok_embeds + pos_embeds\n        x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        logits = self.out_head(x)\n\n        return logits","metadata":{"id":"OaWgG5G8o0_r","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:57:33.326541Z","iopub.execute_input":"2025-05-24T11:57:33.326756Z","iopub.status.idle":"2025-05-24T11:57:33.334879Z","shell.execute_reply.started":"2025-05-24T11:57:33.326737Z","shell.execute_reply":"2025-05-24T11:57:33.334143Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def assign(left, right):\n    if left.shape != right.shape:\n        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n    return torch.nn.Parameter(torch.tensor(right))","metadata":{"id":"7-C1GRlFo079","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:57:33.335599Z","iopub.execute_input":"2025-05-24T11:57:33.335888Z","iopub.status.idle":"2025-05-24T11:57:33.347370Z","shell.execute_reply.started":"2025-05-24T11:57:33.335851Z","shell.execute_reply":"2025-05-24T11:57:33.346772Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import numpy as np\n\ndef load_weights_into_gpt(gpt, params): # Sets the model's positional and token embedding weights to those specified in params.\n    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n\n    for b in range(len(params[\"blocks\"])):\n        q_w, k_w, v_w = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n        gpt.trf_blocks[b].att.W_query.weight = assign(gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n        gpt.trf_blocks[b].att.W_key.weight = assign(gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n        gpt.trf_blocks[b].att.W_value.weight = assign(gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n\n        q_b, k_b, v_b = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n        gpt.trf_blocks[b].att.W_query.bias = assign(gpt.trf_blocks[b].att.W_query.bias, q_b)\n        gpt.trf_blocks[b].att.W_key.bias = assign(gpt.trf_blocks[b].att.W_key.bias, k_b)\n        gpt.trf_blocks[b].att.W_value.bias = assign(gpt.trf_blocks[b].att.W_value.bias, v_b)\n\n        gpt.trf_blocks[b].att.out_proj.weight = assign(gpt.trf_blocks[b].att.out_proj.weight, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n        gpt.trf_blocks[b].att.out_proj.bias = assign(gpt.trf_blocks[b].att.out_proj.bias, params[\"blocks\"][b][\"attn\"][\"c_proj\"] [\"b\"])\n\n        gpt.trf_blocks[b].ff.layers[0].weight = assign(gpt.trf_blocks[b].ff.layers[0].weight, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n        gpt.trf_blocks[b].ff.layers[0].bias = assign(gpt.trf_blocks[b].ff.layers[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n        gpt.trf_blocks[b].ff.layers[2].weight = assign(gpt.trf_blocks[b].ff.layers[2].weight, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n        gpt.trf_blocks[b].ff.layers[2].bias = assign(gpt.trf_blocks[b].ff.layers[2].bias, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n\n        gpt.trf_blocks[b].norm1.scale = assign(gpt.trf_blocks[b].norm1.scale, params[\"blocks\"][b][\"ln_1\"][\"g\"])\n        gpt.trf_blocks[b].norm1.shift = assign(gpt.trf_blocks[b].norm1.shift, params[\"blocks\"][b][\"ln_1\"][\"b\"])\n        gpt.trf_blocks[b].norm2.scale = assign(gpt.trf_blocks[b].norm2.scale, params[\"blocks\"][b][\"ln_2\"][\"g\"])\n        gpt.trf_blocks[b].norm2.shift = assign(gpt.trf_blocks[b].norm2.shift, params[\"blocks\"][b][\"ln_2\"][\"b\"])\n\n    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n","metadata":{"id":"i9wGWvh_plbe","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:57:33.348021Z","iopub.execute_input":"2025-05-24T11:57:33.348268Z","iopub.status.idle":"2025-05-24T11:57:33.360075Z","shell.execute_reply.started":"2025-05-24T11:57:33.348253Z","shell.execute_reply":"2025-05-24T11:57:33.359502Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"BASE_CONFIG = {\n    \"vocab_size\": 50257, # Vocabulary size\n    \"context_length\": 1024, # Context length\n    \"drop_rate\": 0.0, # Dropout rate\n    \"qkv_bias\": True, # Query-key-value bias\n}\nmodel_configs = {\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n}\n\nCHOOSE_MODEL = \"gpt2-medium (355M)\"\nBASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n\nmodel_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EvRzaoJsp3sv","outputId":"1280937b-bf7a-475c-e95f-d805abe18c23","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:57:33.360693Z","iopub.execute_input":"2025-05-24T11:57:33.360882Z","iopub.status.idle":"2025-05-24T11:57:33.373230Z","shell.execute_reply.started":"2025-05-24T11:57:33.360867Z","shell.execute_reply":"2025-05-24T11:57:33.372695Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import urllib.request\n\nurl = \"https://raw.githubusercontent.com/abdussahid26/GPT-Model-from-Scratch-to-Generate-Text/main/gpt_download.py\"\nfilename = url.split('/')[-1]\nurllib.request.urlretrieve(url, filename)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3DK4R5lbqH7M","outputId":"fc93f913-df5a-45b0-cc0f-288947a53d14","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:57:33.373907Z","iopub.execute_input":"2025-05-24T11:57:33.374144Z","iopub.status.idle":"2025-05-24T11:57:33.494468Z","shell.execute_reply.started":"2025-05-24T11:57:33.374114Z","shell.execute_reply":"2025-05-24T11:57:33.493876Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"('gpt_download.py', <http.client.HTTPMessage at 0x7913eb227610>)"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"from gpt_download import download_and_load_gpt2\n\nsettings, params=download_and_load_gpt2(\n    model_size=model_size,\n    models_dir=\"gpt2\"\n)\nmodel=GPTModel(BASE_CONFIG)\nload_weights_into_gpt(model, params)\nmodel.eval()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oz9-ilgyo05L","outputId":"9f66eb57-0c55-4120-9322-807bbf7d6f25","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:57:33.495060Z","iopub.execute_input":"2025-05-24T11:57:33.495261Z","iopub.status.idle":"2025-05-24T11:59:27.680483Z","shell.execute_reply.started":"2025-05-24T11:57:33.495247Z","shell.execute_reply":"2025-05-24T11:59:27.679675Z"}},"outputs":[{"name":"stderr","text":"2025-05-24 11:57:34.970392: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748087855.174779      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748087855.234563      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\ncheckpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 134kiB/s]\nencoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 3.12MiB/s]\nhparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 154kiB/s]\nmodel.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G [01:32<00:00, 15.3MiB/s]\nmodel.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 17.0MiB/s]\nmodel.ckpt.meta: 100%|██████████| 927k/927k [00:00<00:00, 3.17MiB/s]\nvocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 2.17MiB/s]\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"GPTModel(\n  (tok_emb): Embedding(50257, 1024)\n  (pos_emb): Embedding(1024, 1024)\n  (drop_emb): Dropout(p=0.0, inplace=False)\n  (trf_blocks): Sequential(\n    (0): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (1): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (2): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (3): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (4): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (5): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (6): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (7): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (8): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (9): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (10): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (11): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (12): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (13): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (14): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (15): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (16): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (17): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (18): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (19): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (20): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (21): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (22): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (23): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n  )\n  (final_norm): LayerNorm()\n  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n)"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"To ensure that the model was loaded correctly, let's double-check that it generates coherent text.","metadata":{"id":"P2fkLcjJq2lf"}},{"cell_type":"code","source":"def generate_text_simple(model, idx, max_new_tokens, context_size):\n    # idx is (batch, n_tokens) array of indices in the current context\n    for _ in range(max_new_tokens):\n\n        # Crop current context if it exceeds the supported context size\n        # E.g., if LLM supports only 5 tokens, and the context size is 10\n        # then only the last 5 tokens are used as context\n        idx_cond = idx[:, -context_size:]\n\n        # Get the predictions\n        with torch.no_grad():\n            logits = model(idx_cond)\n\n        # Focus only on the last time step\n        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n        logits = logits[:, -1, :]\n\n        # Apply softmax to get probabilities\n        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n\n        # Get the idx of the vocab entry with the highest probability value\n        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n\n        # Append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n\n    return idx","metadata":{"id":"6DB5_afwq_mK","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:59:27.681360Z","iopub.execute_input":"2025-05-24T11:59:27.681882Z","iopub.status.idle":"2025-05-24T11:59:27.687242Z","shell.execute_reply.started":"2025-05-24T11:59:27.681864Z","shell.execute_reply":"2025-05-24T11:59:27.686491Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # Add batch dimension\n    return encoded_tensor\n\ndef token_ids_to_text(token_ids, tokenizer):\n    flat = token_ids.squeeze(0) # remove batch dimension\n    return tokenizer.decode(flat.tolist())","metadata":{"id":"nq35TNmvrnZN","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:59:27.688010Z","iopub.execute_input":"2025-05-24T11:59:27.688527Z","iopub.status.idle":"2025-05-24T11:59:27.701690Z","shell.execute_reply.started":"2025-05-24T11:59:27.688510Z","shell.execute_reply":"2025-05-24T11:59:27.701056Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"text1=\"Every effort moves you\"\n\ntoken_ids=generate_text_simple(\n    model=model,\n    idx=text_to_token_ids(text1, tokenizer),\n    max_new_tokens=15,\n    context_size=BASE_CONFIG[\"context_length\"]\n)\n\nprint(token_ids_to_text(token_ids, tokenizer))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Exin2-Gkro1H","outputId":"0342a14b-9271-41f5-ccf3-8855fd2ce3f4","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:59:27.702447Z","iopub.execute_input":"2025-05-24T11:59:27.702631Z","iopub.status.idle":"2025-05-24T11:59:31.329314Z","shell.execute_reply.started":"2025-05-24T11:59:27.702615Z","shell.execute_reply":"2025-05-24T11:59:31.328489Z"}},"outputs":[{"name":"stdout","text":"Every effort moves you forward, but you must be careful. You must not let your guard down\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"The above output shows that the model generates coherent text, which is an indicator that the model weights are loaded correctly.","metadata":{"id":"rBb6IjCbsDHQ"}},{"cell_type":"markdown","source":"Next, we prepare the model for classification fine-tuning, where we replace the output layer.","metadata":{"id":"noVR4e0EsOVT"}},{"cell_type":"code","source":"torch.manual_seed(123)\n\nnum_classes=2\n\nmodel.out_head=torch.nn.Linear(in_features=1024, out_features=num_classes)\n\ndevice=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gSAgTjz7sVmg","outputId":"322b3685-e743-40ef-a63d-5423940b17e3","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:59:31.330207Z","iopub.execute_input":"2025-05-24T11:59:31.330490Z","iopub.status.idle":"2025-05-24T11:59:32.018941Z","shell.execute_reply.started":"2025-05-24T11:59:31.330473Z","shell.execute_reply":"2025-05-24T11:59:32.018192Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"GPTModel(\n  (tok_emb): Embedding(50257, 1024)\n  (pos_emb): Embedding(1024, 1024)\n  (drop_emb): Dropout(p=0.0, inplace=False)\n  (trf_blocks): Sequential(\n    (0): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (1): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (2): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (3): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (4): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (5): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (6): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (7): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (8): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (9): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (10): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (11): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (12): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (13): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (14): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (15): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (16): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (17): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (18): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (19): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (20): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (21): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (22): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (23): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n  )\n  (final_norm): LayerNorm()\n  (out_head): Linear(in_features=1024, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"# Calculating the classification accuracy\n\ndef calc_accuracy_loader(data_loader, model, device, num_batches=None):\n    model.eval()\n    correct_predictions, num_examples = 0, 0\n    if num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        num_batches = min(num_batches, len(data_loader))\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        if i < num_batches:\n            input_batch = input_batch.to(device)\n            target_batch = target_batch.to(device)\n\n            with torch.no_grad():\n                logits = model(input_batch)[:, -1, :]\n            predicted_labels = torch.argmax(logits, dim=-1)\n            num_examples += predicted_labels.shape[0]\n            correct_predictions += (\n                (predicted_labels == target_batch).sum().item()\n            )\n        else:\n            break\n    return correct_predictions / num_examples","metadata":{"id":"x6r061XBtS-K","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:59:32.019609Z","iopub.execute_input":"2025-05-24T11:59:32.019789Z","iopub.status.idle":"2025-05-24T11:59:32.025527Z","shell.execute_reply.started":"2025-05-24T11:59:32.019776Z","shell.execute_reply":"2025-05-24T11:59:32.024885Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"torch.manual_seed(123)\n\ntrain_accuracy=calc_accuracy_loader(train_loader, model, device, num_batches=10)\nval_accuracy=calc_accuracy_loader(val_loader, model, device, num_batches=10)\ntest_accuracy=calc_accuracy_loader(test_loader, model, device, num_batches=10)\n\nprint(f\"Training accuracy: {train_accuracy * 100:.2f}%\")\nprint(f\"Validation accuracy: {val_accuracy * 100:.2f}%\")\nprint(f\"Test accuracy: {test_accuracy * 100:.2f}%\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g1-VX1GRtywF","outputId":"f3da80db-3e9f-4eee-f477-4d077b9f7469","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:59:32.026130Z","iopub.execute_input":"2025-05-24T11:59:32.026386Z","iopub.status.idle":"2025-05-24T11:59:37.692182Z","shell.execute_reply.started":"2025-05-24T11:59:32.026363Z","shell.execute_reply":"2025-05-24T11:59:37.691561Z"}},"outputs":[{"name":"stdout","text":"Training accuracy: 46.25%\nValidation accuracy: 45.00%\nTest accuracy: 48.75%\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"### **Stage 3: Parameter-efficient fine-tuning with LoRA**\n\n\n### Weight Update Equations\n\n1. **Basic weight update:**\n\n   $W_{\\text{updated}} = W + \\Delta W$\n\n2. **Using matrix decomposition:**\n\n   $W_{\\text{updated}} = W + AB$\n\n3. **Applying input $x$:**\n\n   $W_{\\text{updated}} = xW + xAB$\n","metadata":{"id":"OS3cKGpzwC6M"}},{"cell_type":"code","source":"# Implementing a LoRA layer\n\n# Calculating: xAB\n\nimport math\nimport torch.nn as nn\n\nclass LoRALayer(nn.Module):\n    def __init__(self, in_dim, out_dim, rank, alpha):\n        super().__init__()\n        self.A=torch.nn.Parameter(torch.empty(in_dim, rank))\n        torch.nn.init.kaiming_uniform(self.A, a=math.sqrt(5))\n        self.B=torch.nn.Parameter(torch.zeros(rank, out_dim))\n        self.alpha=alpha\n\n    def forward(self, x):\n        x=self.alpha * (x @ self.A @ self.B)\n        return x","metadata":{"id":"ijqHxWLmwKNy","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:59:37.692936Z","iopub.execute_input":"2025-05-24T11:59:37.693353Z","iopub.status.idle":"2025-05-24T11:59:37.698424Z","shell.execute_reply.started":"2025-05-24T11:59:37.693332Z","shell.execute_reply":"2025-05-24T11:59:37.697692Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Calculating: xW + xAB\n\nclass LinearWithLoRA(nn.Module):\n    def __init__(self, linear, rank, alpha):\n        super().__init__()\n        self.linear=linear\n        self.lora=LoRALayer(linear.in_features, linear.out_features, rank, alpha)\n\n    def forward(self, x):\n        return self.linear(x) + self.lora(x)","metadata":{"id":"1h0qKslV3KJC","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:59:37.699266Z","iopub.execute_input":"2025-05-24T11:59:37.699498Z","iopub.status.idle":"2025-05-24T11:59:37.709592Z","shell.execute_reply.started":"2025-05-24T11:59:37.699482Z","shell.execute_reply":"2025-05-24T11:59:37.709061Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def replace_linear_with_lora(model, rank, alpha):\n    for name, module in model.named_children():\n        if isinstance(module, nn.Linear):\n            setattr(model, name, LinearWithLoRA(module, rank, alpha)) # Replaces the linear layer with LinearWithLoRA\n        else:\n            replace_linear_with_lora(module, rank, alpha) # Recursively applies the same function to child modules","metadata":{"id":"V8jCF1Lc45GI","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:59:37.710263Z","iopub.execute_input":"2025-05-24T11:59:37.710913Z","iopub.status.idle":"2025-05-24T11:59:37.722727Z","shell.execute_reply.started":"2025-05-24T11:59:37.710890Z","shell.execute_reply":"2025-05-24T11:59:37.722206Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"Before we apply the LinearWithLoRA layer upgrades, we first freeze the original model parameters.","metadata":{"id":"77X0qE0w54tS"}},{"cell_type":"code","source":"total_params1=sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total trainable parameters before: {total_params1:,}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AJ7sEp4y53gl","outputId":"34159976-68cc-4a3f-8490-6806bde6cfdb","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:59:37.723529Z","iopub.execute_input":"2025-05-24T11:59:37.723704Z","iopub.status.idle":"2025-05-24T11:59:37.736202Z","shell.execute_reply.started":"2025-05-24T11:59:37.723683Z","shell.execute_reply":"2025-05-24T11:59:37.735378Z"}},"outputs":[{"name":"stdout","text":"Total trainable parameters before: 354,825,218\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad=False\n\ntotal_params2=sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total trainable parameters after: {total_params2:,}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yEjsgHMU6TXm","outputId":"181fe583-7b01-4a45-fd0d-cc1b0aa5db30","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:59:37.736824Z","iopub.execute_input":"2025-05-24T11:59:37.737014Z","iopub.status.idle":"2025-05-24T11:59:37.750398Z","shell.execute_reply.started":"2025-05-24T11:59:37.736999Z","shell.execute_reply":"2025-05-24T11:59:37.749733Z"}},"outputs":[{"name":"stdout","text":"Total trainable parameters after: 0\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"Next, we use the `replace_linear_with_lora` to replace the `Linear` layers.","metadata":{"id":"GkVFpsGe6xKv"}},{"cell_type":"code","source":"replace_linear_with_lora(model, rank=16, alpha=16) # rank and alpha of 16 are good default choices.\ntotal_params3=sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total trainable LoRA parameters: {total_params3:,}\")\nprint(f\"Only uses {total_params3/total_params1 * 100}% of total {total_params1:,} parameters\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tsGrYiM465sb","outputId":"d225ba67-b6a8-413b-8da7-d98b9ab6feab","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:59:37.751567Z","iopub.execute_input":"2025-05-24T11:59:37.751837Z","iopub.status.idle":"2025-05-24T11:59:37.804002Z","shell.execute_reply.started":"2025-05-24T11:59:37.751813Z","shell.execute_reply":"2025-05-24T11:59:37.803367Z"}},"outputs":[{"name":"stdout","text":"Total trainable LoRA parameters: 7,094,304\nOnly uses 1.9993798749670606% of total 354,825,218 parameters\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_35/613958189.py:12: FutureWarning: `nn.init.kaiming_uniform` is now deprecated in favor of `nn.init.kaiming_uniform_`.\n  torch.nn.init.kaiming_uniform(self.A, a=math.sqrt(5))\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"Let's verify that the layers have been modified as intended by printing the model architecture.","metadata":{"id":"j7oEFnxk8ry-"}},{"cell_type":"code","source":"device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel.to(device)\nprint(model)","metadata":{"id":"6yig1gIs8x5U","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:59:37.804916Z","iopub.execute_input":"2025-05-24T11:59:37.805248Z","iopub.status.idle":"2025-05-24T11:59:37.837749Z","shell.execute_reply.started":"2025-05-24T11:59:37.805224Z","shell.execute_reply":"2025-05-24T11:59:37.837197Z"}},"outputs":[{"name":"stdout","text":"GPTModel(\n  (tok_emb): Embedding(50257, 1024)\n  (pos_emb): Embedding(1024, 1024)\n  (drop_emb): Dropout(p=0.0, inplace=False)\n  (trf_blocks): Sequential(\n    (0): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (1): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (2): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (3): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (4): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (5): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (6): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (7): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (8): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (9): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (10): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (11): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (12): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (13): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (14): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (15): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (16): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (17): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (18): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (19): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (20): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (21): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (22): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (23): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n  )\n  (final_norm): LayerNorm()\n  (out_head): LinearWithLoRA(\n    (linear): Linear(in_features=1024, out_features=2, bias=True)\n    (lora): LoRALayer()\n  )\n)\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"Let's fine-tune the model using LoRA.","metadata":{"id":"8gsJgONi9G2j"}},{"cell_type":"code","source":"def calc_loss_batch(input_batch, target_batch, model, device):\n    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n    logits = model(input_batch)[:,-1,:] # Logits of last output token\n    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n    return loss\n\ndef calc_loss_loader(data_loader, model, device, num_batches=None):\n    total_loss = 0.\n    if len(data_loader) == 0:\n        return float(\"nan\")\n    elif num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        # Reduce the number of batches to match the total number of batches in the data loader\n        # if num_batches exceeds the number of batches in the data loader\n        num_batches = min(num_batches, len(data_loader))\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        if i < num_batches:\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            total_loss += loss.item()\n        else:\n            break\n    return total_loss / num_batches","metadata":{"id":"KKAzkphL_w5I","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:59:37.838468Z","iopub.execute_input":"2025-05-24T11:59:37.839003Z","iopub.status.idle":"2025-05-24T11:59:37.844002Z","shell.execute_reply.started":"2025-05-24T11:59:37.838987Z","shell.execute_reply":"2025-05-24T11:59:37.843465Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n    model.eval()\n    with torch.no_grad():\n        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n    model.train()\n    return train_loss, val_loss","metadata":{"id":"-NUAO77QAyHY","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:59:37.844659Z","iopub.execute_input":"2025-05-24T11:59:37.844920Z","iopub.status.idle":"2025-05-24T11:59:37.859675Z","shell.execute_reply.started":"2025-05-24T11:59:37.844897Z","shell.execute_reply":"2025-05-24T11:59:37.858918Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def finetuning(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter):\n    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n    examples_seen, global_step = 0, -1\n\n    for epoch in range(num_epochs):\n        model.train()\n        for input_batch, target_batch in train_loader:\n            optimizer.zero_grad() # Reset loss gradients from the previous batch iteration\n            loss=calc_loss_batch(input_batch, target_batch, model, device)\n            loss.backward()  # Calculate loss gradients\n            optimizer.step() # Update model weights using loss gradients\n            examples_seen += input_batch.shape[0]\n            global_step += 1\n\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model,\n                    train_loader,\n                    val_loader,\n                    device,\n                    eval_iter\n                )\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n\n        train_accuracy=calc_accuracy_loader(\n            train_loader,\n            model,\n            device,\n            num_batches=eval_iter\n        )\n        val_accuracy=calc_accuracy_loader(\n            val_loader,\n            model,\n            device,\n            num_batches=eval_iter\n        )\n\n        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n\n        train_accs.append(train_accuracy)\n        val_accs.append(val_accuracy)\n\n    return train_losses, val_losses, train_accs, val_accs, examples_seen\n\n","metadata":{"id":"AqttXawp-LH5","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:59:37.860577Z","iopub.execute_input":"2025-05-24T11:59:37.860799Z","iopub.status.idle":"2025-05-24T11:59:37.874009Z","shell.execute_reply.started":"2025-05-24T11:59:37.860776Z","shell.execute_reply":"2025-05-24T11:59:37.873338Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# Fine-tuning a model with LoRA layers\ntorch.manual_seed(123)\n\nimport time\n\nstart_time=time.time()\noptimizer=torch.optim.AdamW(params=model.parameters(), lr=5e-5, weight_decay=0.1)\n\nnum_epochs=5\n\ntrain_losses, val_losses, train_accs, val_accs, examples_seen=finetuning(\n    model,\n    train_loader,\n    val_loader,\n    optimizer,\n    device,\n    num_epochs=num_epochs,\n    eval_freq=50,\n    eval_iter=5,\n)\n\nend_time=time.time()\n\nexecution_time_minutes=(end_time-start_time)/60\nprint(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n","metadata":{"id":"4JDo4E2e9Mc2","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T11:59:37.874709Z","iopub.execute_input":"2025-05-24T11:59:37.874920Z","iopub.status.idle":"2025-05-24T12:05:55.975277Z","shell.execute_reply.started":"2025-05-24T11:59:37.874906Z","shell.execute_reply":"2025-05-24T12:05:55.974635Z"}},"outputs":[{"name":"stdout","text":"Ep 1 (Step 000000): Train loss 2.048, Val loss 1.806\nEp 1 (Step 000050): Train loss 0.329, Val loss 0.276\nEp 1 (Step 000100): Train loss 0.098, Val loss 0.530\nTraining accuracy: 97.50% | Validation accuracy: 95.00%\nEp 2 (Step 000150): Train loss 0.194, Val loss 0.091\nEp 2 (Step 000200): Train loss 0.004, Val loss 0.089\nEp 2 (Step 000250): Train loss 0.058, Val loss 0.356\nTraining accuracy: 90.00% | Validation accuracy: 95.00%\nEp 3 (Step 000300): Train loss 0.099, Val loss 0.068\nEp 3 (Step 000350): Train loss 0.014, Val loss 0.100\nTraining accuracy: 100.00% | Validation accuracy: 97.50%\nEp 4 (Step 000400): Train loss 0.017, Val loss 0.114\nEp 4 (Step 000450): Train loss 0.009, Val loss 0.113\nEp 4 (Step 000500): Train loss 0.003, Val loss 0.106\nTraining accuracy: 100.00% | Validation accuracy: 97.50%\nEp 5 (Step 000550): Train loss 0.001, Val loss 0.113\nEp 5 (Step 000600): Train loss 0.002, Val loss 0.106\nTraining accuracy: 95.00% | Validation accuracy: 97.50%\nTraining completed in 6.30 minutes.\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"Let's visualize the loss curves to better see whether the training has converged.","metadata":{"id":"wZ7h527vCWYd"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n    fig, ax1 = plt.subplots(figsize=(5, 3))\n\n    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n    ax1.plot(\n        epochs_seen, val_values, linestyle=\"-.\",\n        label=f\"Validation {label}\"\n    )\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(label.capitalize())\n    ax1.legend()\n\n    ax2 = ax1.twiny()\n    ax2.plot(examples_seen, train_values, alpha=0)\n    ax2.set_xlabel(\"Examples seen\")\n\n    fig.tight_layout()\n    plt.savefig(f\"{label}-plot.pdf\")\n    plt.show()","metadata":{"id":"YC6Ahg03CXKQ","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T12:05:55.975950Z","iopub.execute_input":"2025-05-24T12:05:55.976506Z","iopub.status.idle":"2025-05-24T12:05:55.981748Z","shell.execute_reply.started":"2025-05-24T12:05:55.976486Z","shell.execute_reply":"2025-05-24T12:05:55.981071Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\nexamples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n\nplot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses, label=\"loss\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"ruer8-JWCyTX","outputId":"e3355673-d84a-4b49-bf23-5f1c262a2ac4","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T12:05:55.982470Z","iopub.execute_input":"2025-05-24T12:05:55.982727Z","iopub.status.idle":"2025-05-24T12:05:57.497561Z","shell.execute_reply.started":"2025-05-24T12:05:55.982706Z","shell.execute_reply":"2025-05-24T12:05:57.496860Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 500x300 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"train_accuracy = calc_accuracy_loader(train_loader, model, device)\nval_accuracy = calc_accuracy_loader(val_loader, model, device)\ntest_accuracy = calc_accuracy_loader(test_loader, model, device)\n\nprint(f\"Training accuracy: {train_accuracy*100:.2f}%\")\nprint(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\nprint(f\"Test accuracy: {test_accuracy*100:.2f}%\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"AYg4bNV3C7XT","outputId":"55b836ed-63f0-4880-edff-8aebd2c0ac38","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T12:05:57.498232Z","iopub.execute_input":"2025-05-24T12:05:57.498700Z","iopub.status.idle":"2025-05-24T12:06:39.248117Z","shell.execute_reply.started":"2025-05-24T12:05:57.498683Z","shell.execute_reply":"2025-05-24T12:06:39.247421Z"}},"outputs":[{"name":"stdout","text":"Training accuracy: 97.88%\nValidation accuracy: 98.66%\nTest accuracy: 93.67%\n","output_type":"stream"}],"execution_count":39}]}